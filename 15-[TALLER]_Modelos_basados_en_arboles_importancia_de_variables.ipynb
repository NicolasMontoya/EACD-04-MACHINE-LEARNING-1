{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-04-MACHINE-LEARNING-1/blob/master/15-%5BTALLER%5D_Modelos_basados_en_arboles_importancia_de_variables.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "**Recuerda que una vez abierto, Da clic en \"Copiar en Drive\", de lo contrario no podras alamancenar tu progreso**\n",
    "\n",
    "Nota: no olvide ir ejecutando las celdas de código de arriba hacia abajo para que no tenga errores de importación de librerías o por falta de definición de variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget --no-cache -O init.py -q https://raw.githubusercontent.com/UDEA-Esp-Analitica-y-Ciencia-de-Datos/EACD-04-MACHINE-LEARNING-1/master/init.py\n",
    "import init; init.init(force_download=False); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1.\n",
    "\n",
    "**Bag of words + Bagging**\n",
    "\n",
    "\n",
    "## Contextualización del problema\n",
    "\n",
    "\n",
    "Usaremos el dataset Twitter US Airline Sentiment para reesolver un problema de análisis de sentimiento en texto. En el repositorio de Kaggle se encuentra más información en el siguiente [link](https://www.kaggle.com/crowdflower/twitter-airline-sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('local/data/Tweets.csv')\n",
    "# Keeping only the neccessary columns\n",
    "data = data[['text','airline_sentiment']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como es un problema de procesamiento de texto debemos hacer algunas tareas báscias de preprocesamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#Remove neutral class\n",
    "data = data[data.airline_sentiment != \"neutral\"]\n",
    "\n",
    "#text normalization\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x:re.sub('@[^\\s]+','',x)))#remove the name of the airline\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "print(f\"Número de muestras positivas {np.sum(data['airline_sentiment'].values == 'positive')}\")\n",
    "print(f\"Número de muestras negatias {np.sum(data['airline_sentiment'].values == 'negative')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,row in data.iterrows():\n",
    "    row[0] = row[0].replace('rt',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma más básica de representar textos para problemas de clasificación es usar la estrategia Bag of Words [link](https://en.wikipedia.org/wiki/Bag-of-words_model#:~:text=The%20bag%2Dof%2Dwords%20model,word%20order%20but%20keeping%20multiplicity.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "count_vectorizer = CountVectorizer(max_features=2000, stop_words='english')\n",
    "\n",
    "%time count_matrix = count_vectorizer.fit_transform(data['text']) #fit the vectorizer to synopses\n",
    "\n",
    "print(count_matrix.shape)\n",
    "count_matrix.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.1: Clasificación usando estrategias de Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realice la partición de los datos entre Entrenamiento y Test. Recuerde que Test es el subconjunto que dejará pára medir el desempeño del sistema después del entrenamiento y de la selección de los hiperparámetros. \n",
    "\n",
    "Teniendo en cuenta la distribución de clases, seleccione correctamente la estrategia de particionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "#Complete el código\n",
    "X_train, X_test, y_train, y_test = train_test_split(..., test_size=0.2, random_state=42, stratify=...)\n",
    "\n",
    "st = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diseñe un experimento para ajustar los hiper-parámetros de un modelo Random Forest. Ajuste el número de árboles, la profundida de cada árbol y el número de variables a analizar por nodo. Recuerde que como el problema es desbalanceado debe usar el parámetro **class_weight='balanced_subsample'** para la definición del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_b = ... #use random_state=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_estimators':[20,40,60,80,100,120], 'max_depth':[2,4,6,8], 'max_features':[10,20,30,40,50]}\n",
    "\n",
    "clf = GridSearchCV(estimator=..., param_grid=parameters, cv=st, scoring='balanced_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe en las muestras de test. Estime: Accuracy, Accuracy Balanceado y F1. Grafique la matriz de confusión normalizada. Para eso ejecute la siguiente celda. Analice el código usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score, plot_confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder().fit(np.unique(data['airline_sentiment']))\n",
    "\n",
    "Yest = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.2\n",
    "\n",
    "Diseñe un experimento para ajustar los hiper-parámetros de un modelo de Bagging de árboles de decisión. Ajuste el número de árboles y la profundida de cada árbol.\n",
    "\n",
    "Tenga en cuenta que el GridSearch no puede modificar la profundida del árbol de decisión porque sólo puede acceder a los parámetros del estimador, que en este caso es un modelo de Bagging, la profundidad hace parte de los párametros del estimador base y no son accesibles por el GridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth = [2,4,6,8]\n",
    "parameters = {'n_estimators':[20,40,60,80,100]}\n",
    "clf2 = []\n",
    "for i in max_depth:\n",
    "    #Complete el código\n",
    "    clf_b = ...\n",
    "    clf_bagging = BaggingClassifier(base_estimator=clf_b, random_state=0)\n",
    "    clf2.append(GridSearchCV(estimator=clf_bagging, param_grid=parameters, cv=st, scoring='balanced_accuracy').fit(X_train,y_train))\n",
    "\n",
    "best_score = 0\n",
    "for i in range(len(max_depth)):\n",
    "    if clf2[i].best_score_ >= best_score:\n",
    "        best_score = clf2[i].best_score_\n",
    "        ind = i\n",
    "clf2 = clf2[ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe en las muestras de test. Estime Accuracy, Accuracy Balanceado y F1. Grafique la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest = clf2.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf2, X_test, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1.3: Importancia de variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafique la importancia de las variables de acuerdo con el mejor de los dos comités de máquina evaluados. Determine cuántas variables tienen importancia 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "plt.bar(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cuáles son las 10 palabras más determinantes para definir el sentimiento de los tweets? Incluya el código necesario para poder responder la pregunta.\n",
    "\n",
    "**Ayuda**: count_vectorizer.vocabulary_ contiene el diccionario de palabras usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cree nueva versiones para los conjuntos Xtrain y Xtest, eliminando las variables que según el análisis anterior tienen importancia cero. Realice nuevamente el proceso de entrenamiento y validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejercicio de código\n",
    "X_train2 = X_train[:,...]\n",
    "X_test2 = X_test[:,...]\n",
    "\n",
    "\n",
    "parameters = ...\n",
    "clf_b = ...\n",
    "clf3 = GridSearchCV(estimator=clf_b, param_grid=parameters, cv=st, scoring=...)\n",
    "clf3.fit(X_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3.best_estimator_.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe en las muestras de test. Estime Accuracy, Accuracy Balanceado y F1. Grafique la matriz de confusión. ¿Cambió el resultado?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest = clf3.predict(X_test2)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf3, X_test2, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Pregunta Abierta\n",
    "#@markdown  ¿Cambió el resultado del modelo de manera significativa al reducir el número de variables?\n",
    "respuesta_1 = \"\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2.\n",
    "\n",
    "**tf-idf + Boosting**\n",
    "\n",
    "En este caso vamos a usar una estrategia un poco más robusta para la caracterización de los textos, se llama tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf representation\n",
    "\n",
    "This stands for term frequency and inverse document frequency. The tf-idf weighting scheme assigns to term $t$ a weight in document $d$ given by\n",
    "\n",
    "$$\n",
    "\\mbox{tf-idf}_{t,d} = \\mbox{tf}_{t,d} \\times \\mbox{idf}_t.$$\n",
    "\n",
    "**Term Frequency (tf)**: gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document. It increases as the number of occurrences of that word within the document increases. Each document has its own tf.\n",
    "\n",
    "$$\\mbox{tf}_{t,d} = \\frac{n_{t,d}}{\\sum_k n_{k,d}} $$\n",
    "\n",
    "**Inverse Data Frequency (idf)**: used to calculate the weight of rare words across all documents in the corpus. The words that occur rarely in the corpus have a high IDF score. It is given by the equation below.\n",
    "\n",
    "$$\\mbox{idf}_{t} = \\log\\left(\\frac{N}{df_t}\\right) + 1; \\; df_t = \\text{number of documents contaning } t $$\n",
    "\n",
    "In other words, $\\mbox{tf-idf}_{t,d}$ assigns to term $t$ a weight in document $d$ that is\n",
    "\n",
    "- highest when $t$ occurs many times within a small number of documents (thus lending high discriminating power to those documents);\n",
    "- lower when the term occurs fewer times in a document, or occurs in many documents (thus offering a less pronounced relevance signal);\n",
    "\n",
    "- lowest when the term occurs in virtually all documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#define vectorizer parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2000,stop_words='english',\n",
    "                                 use_idf=True) #ngram_range=(1,3)\n",
    "\n",
    "%time tfidf_matrix = tfidf_vectorizer.fit_transform(data['text']) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix.shape)\n",
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: Solo de carácter informativo. El proceso de construcción de la matriz tf-ids se puede realizar de manera alternativa usando la clase TfidfTransformer a partir de la matrix de conteos construida en el punto 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "tfidf_matrix2 = tfidf_transformer.fit_transform(count_matrix) #fit the vectorizer to synopses\n",
    "\n",
    "print(tfidf_matrix2.shape)\n",
    "print(tfidf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2.1: Clasificación usando estrategias de Boosting\n",
    "\n",
    "Realice la partición de los datos entre Entrenamiento y Test. Recuerde que Test es el subconjunto que dejará pára medir el desempeño del sistema después del entrenamiento y de la selección de los hiperparámetros. \n",
    "\n",
    "Teniendo en cuenta la distribución de clases, seleccione correctamente la estrategia de particionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete el código\n",
    "X_train, X_test, y_train, y_test = train_test_split(..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diseñe un experimento para ajustar los hiper-parámetros de un modelo Gradient Boosting Tree. Ajuste el número de árboles, la profundida de cada árbol y el número de variables a analizar por nodo.\n",
    "\n",
    "**Note** que en este caso el modelo GBT no cuenta con un parámetro para cambiar el peso que cada clase tiene en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete el código\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf_b = GradientBoostingClassifier(random_state=0)\n",
    "\n",
    "parameters = ...\n",
    "\n",
    "clf = GridSearchCV(estimator=clf_b, param_grid=parameters, cv=st, scoring=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_.n_estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe en las muestras de test. Estime Accuracy, Accuracy Balanceado y F1. Grafique la matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf, X_test, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2.2: \n",
    "\n",
    "Modifique el experimento anterior para ajustar los hiper-parámetros de un modelo **Stochastic** Gradient Boosting Tree. Ajuste el número de árboles, la profundida de cada árbol y el número de variables a analizar por nodo. Use un parámetro subsample = 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete el código\n",
    "clf_b = GradientBoostingClassifier(...\n",
    "\n",
    "parameters = ...\n",
    "\n",
    "clf2 = GridSearchCV(estimator=clf_b, param_grid=parameters, cv=st, scoring=...)\n",
    "clf2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evalúe en las muestras de test. Estime: Accuracy, Accuracy Balanceado y F1. Grafique la matriz de confusión normalizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest = clf2.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf2, X_test, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2.3\n",
    "\n",
    "Determine las 10 palabras más importantes para la clasificación de acuerdo con el mejor modelo de la parte 2.\n",
    "\n",
    "**Ayuda**: tfidf_vectorizer.vocabulary_ contiene el diccionario de palabras usado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete el código\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Ejercicio 3: XGBoost\n",
    "\n",
    "Se recomienda revisar la documentación oficial [link](https://xgboost.readthedocs.io/en/latest/index.html). XGBoost tiene una sintaxis diferente a sklearn pero incluye un wrapper que permite usarlo igual que un modelo estándar de sklearn y por lo tanto permite usar GridSearch para realizar el ajuste de los hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost tiene muchos más ṕarámetros que el GBT estándar. Además del número de estimadores, la profundidad del árbol y la tasa de aprendizaje (también en el GBT), XGBoost contiene parámetros de regularización $l_1$ y $l_2$ y a través del parámetro scale_pos_weight sepuede modificar el peso de la clase positiva en relación con la negativa.\n",
    "\n",
    "Analice la manera en que está ajustado ese parámetro en el siguiente código:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight = compute_class_weight('balanced',le.classes_,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'n_estimators':[20,40,60,80,100,120,140],'max_depth': [2,4,6,8], 'learning_rate':[1,0.5,0.1], 'reg_lambda':[0.1,0.2]}\n",
    "clf_b = xgb.XGBClassifier(scale_pos_weight=class_weight[1]/class_weight[0],random_state = 0, objective='binary:logistic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = GridSearchCV(estimator=clf_b, param_grid=param, cv=st, scoring='balanced_accuracy')\n",
    "clf3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yest = clf3.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy = {accuracy_score(y_test,Yest)}\")\n",
    "print(f\"Balanced Accuracy = {balanced_accuracy_score(y_test,Yest)}\")\n",
    "\n",
    "#Las métricas F1, precision and recall requieren que se establezca la convención de cuál es la clase positiva (1)\n",
    "print(f\"F1 = {f1_score(le.transform(y_test),le.transform(Yest))}\")\n",
    "\n",
    "disp = plot_confusion_matrix(clf3, X_test, y_test, display_labels=np.unique(data['airline_sentiment']),\n",
    "                             cmap=plt.cm.Blues, \n",
    "                             normalize='true')\n",
    "disp.ax_.set_title('MC normalizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Igual que para los casos anteriores determine el número de características con importancia diferente de cero, y las 10 palabras más importantes según el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete el código\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
